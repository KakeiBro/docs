= CI/CD Pipelines

First of all, the pipelines will be built using Github Actions, and they will be 
all the way to deployment (Continous Integration/Continuous **Deployment**). With 
that in mind we will break down the phylosophy of how they are setup and key points 
to take into consideration.

[NOTE]
====
By convention pipelines are setup in `.yml` files that are comitted to the repository. 
They should be all under `.github/workflows/`.
====

== Philosophy

A robust CI/CD pipeline for any modern application is crucial for ensuring smooth 
development, testing, and deployment processes. This specific project takes both 
personal experience, but also nuggets of wisdom coming from across experts in the field, 
a source for the ideas applied is: https://www.amazon.com/Hands-Continuous-Integration-Delivery-software-ebook/dp/B07F2KCM75/ref=sr_1_1?crid=BDMDGLVNFVVG&dib=eyJ2IjoiMSJ9.v9neNylqRvqVl8MRAQV2Eetm9ahsqk0XnKJIYkuCKQRFYSRwsoC2KDl40AHiyOX2j5h-7-FzRIqbFwfDJwPaakrOYH_NoHUZud6Ll1BkUmAq0CWLJboij5BqgGKZom-n.lyWaPFA_05QDnljjt2BUT4ykH4G5eTElz8-jDr7zJjQ&dib_tag=se&keywords=Hands-On+Continuous+Delivery&qid=1739626506&sprefix=hands-on+continuous+deliver%2Caps%2C182&sr=8-1[Hands-On Continuous Integration and Delivery].

In short, manual processes, and desorganized processes should be mitigated as much 
as possible, and it's through communication with different areas of the business that we 
can find areas of improvement. **Automation** should be a key word when it comes to 
relegating processes that are grunt-work in nature and that only take away from more 
important tasks for a developer. Besides that, we should have a clear flow of work 
in order for the software we build to be fast on delivery and health checking. If 
someone coded something that breaks the system, we should pick up on that as quickly 
as possible.

Whilst we already have put a set of automation steps focused on each workstation through 
xref:husky.adoc[Husky], there are definitely many ways to circumvent those, and we shouldn't 
have to rely entirely on each developer's willingness to adhere to protocol but a centralized 
and (in theory) incorruptible source of thruth. Hence there are other mechanisms to 
keep tabs on code quality and correctness (according to a in-house set of rules).

- Pull Requests
- CI/CD Pipelines

Due to the nature of this project, **pull requests** will definitely won't be leveraged 
as much, unless the need or opportunity for them arises, however they should be still 
be in place so that a whole team can have a look at code changes that are about to be 
integrated into the codebase and give feedback, and also pick up on possible issues 
or opportunities for improvement.

But, **Pipelines**, on the other hand will be leveraged in the form of **GitHub Actions**, 
there will be one pipeline for **PRs** and another pipeline for CI in **main**. (It's 
here that you can see that PRs and Pipelines should also come in hand and have arguments 
when it comes to trimming down checks for one use case and then adding others for another).

Even if a developer has already go through the **layers we have setup** in the form of 
**hooks**, we will have pipelines with **some of the same steps** plus others specific 
to it so that the process is faster when running on the Remote Repository platform (GitHub).

The idea behind **Continous Deployment** is _Agile_ in nature. Always be building 
something, testing, and publishing it. It will depend highly on the _team and product_ 
if you want to get up until that point, since there might be justified use cases in 
which you only want to go all the way to **Continous Delivery**. And by some manual 
intervention, the deployment has to be done by someone assigned to said task. 

**In our case**, we are going all the way to deployment, but be aware that this is 
extremely taxing in nature, since we are running on a build machine. So be sure to 
optimize when pipelines should run and not incurr in them becoming a hassle and source 
of frustation, when they should be there to make things easier.

[NOTE]
====
And in that endeavor of making the pipelines to be as fast as possible, we will leverage 
caching, and specifically caching with `pnpm`. PNPM is already a tool aimed at 
reusing node modules if we have them across multiple projects, and it also has optimized 
performance built-into it. With that plus a correct usage of **module caching** we 
can aim at having cold starts for our frontend builds, but later builds to be much faster 
since they will leverage all previously installed node modules, instead of having 
to always install them from scratch.
====

== PR Pipeline

This consists in **one job** called "Lint, Build, Check". **It will only run on PRs pointed at master**.

=== CI

Steps:

* Run on a base Ubuntu image, and from the root of the repo, head down to the `./kakeibro-web` folder.
* Establish a strategy: Run in parallel two instances of the pipeline (_matrix_), 
one with Node 20 and another one with Node 22
** _A good practice is to test out the build process in different node versions_. 
Since we are following xref:ROOT:onboarding/index.adoc#philosophy[best practices], 
we should be testing on the _Active LTS_ and _Active Maintenance_ versions of node.
* We checkout the code with a pre-defined `actions/checkout@v4` action
** _Note:_ At the time of writing the pre-defined actions are at **v4** this could 
end up getting updated later, and we should be mindful of this in case there are 
actions that are still on other versions such as **v3**.
** _Note 2:_ We have to use these pre-defined sub-routines since github encourages it 
and are meant to **abstract** really general use cases such as cloning the repo first, 
(the build machine will always start with an empty state), they get the added bonus 
of having other syntax that can build on top of the base functionality without us having 
to code it from scratch (e.g: Fetch specific commits, optimize performance, attaching submodules)
* We install `pnpm` with `pnpm/action-setup@v4`. Just like the previous step it will 
work with a pre-defined script aimed at abstracting logic and making the file look cleaner
* We install node with `actions/setup-node@v4`, in this specific instance we want to tie 
it to `pnpm`, and due to the _matrix_ we setup at the beginning we have to now query the `matrix.node-version` 
variable in order to accomodate node with the respective node version, besides that we can setup 
**caching** by simply pointing at it to use a strategy that caches with `pnpm`. We 
also have to point specifically to the `pnpm-lock.yaml` file so that the pipeline and the 
build machine can read it and check their own cache of packages and if all the hashes match 
it will then use the cache instead of trying to install everything again from scratch
** **IMPORTANT:** It is for these types of use cases that comitting the `-lock` file can 
be extremely helpful.
* We install all the dependencies with `pnpm` and with the `--frozen-lockfile` flag so 
that it doesn't try to overwrite something on the lockfile.
** _Note:_ `pnpm i --frozen-lockfile` is recommended to be used on CI/CD pipelines, to ensure 
consistency. The command install dependencies without modifying the `pnpm-lock.yaml` 
file.
* We then run **ESLint** by leveraging the scripts that are also leveraged by xref:husky.adoc[Husky].
* We then run a **build** to check that all the code is okay and nothing broke.
* We then leverage a script called `debug-check.js` that's under a `scripts` folder at 
the web app folder. This is used by both _Husky_ and now the CI/CD pipeline in order to 
check for `console.log` or `debugger` statements in the code and failing if it does find 
them.
* We then run a **check for outdated packages**, but we fail silently, still the table of 
all dependencies that are outdated will show in the pipeline summary.
* We will then run a script that **checks for package versions that have vulnerabilities** 
in them that were found.

== Master Pipeline

This consists of **two jobs**, "Lint, Build, Check", "Deploy to Firebase"

=== CI

Steps:

* Run on a base Ubuntu image, and from the root of the repo, head down to the `./kakeibro-web` folder.
* Establish a strategy: Run in parallel two instances of the pipeline (_matrix_), 
one with Node 20 and another one with Node 22
** _A good practice is to test out the build process in different node versions_. 
Since we are following xref:ROOT:onboarding/index.adoc#philosophy[best practices], 
we should be testing on the _Active LTS_ and _Active Maintenance_ versions of node.
* We checkout the code with a pre-defined `actions/checkout@v4` action
** _Note:_ At the time of writing the pre-defined actions are at **v4** this could 
end up getting updated later, and we should be mindful of this in case there are 
actions that are still on other versions such as **v3**.
** _Note 2:_ We have to use these pre-defined sub-routines since github encourages it 
and are meant to **abstract** really general use cases such as cloning the repo first, 
(the build machine will always start with an empty state), they get the added bonus 
of having other syntax that can build on top of the base functionality without us having 
to code it from scratch (e.g: Fetch specific commits, optimize performance, attaching submodules)
* We install `pnpm` with `pnpm/action-setup@v4`. Just like the previous step it will 
work with a pre-defined script aimed at abstracting logic and making the file look cleaner
* We install node with `actions/setup-node@v4`, in this specific instance we want to tie 
it to `pnpm`, and due to the _matrix_ we setup at the beginning we have to now query the `matrix.node-version` 
variable in order to accomodate node with the respective node version, besides that we can setup 
**caching** by simply pointing at it to use a strategy that caches with `pnpm`. We 
also have to point specifically to the `pnpm-lock.yaml` file so that the pipeline and the 
build machine can read it and check their own cache of packages and if all the hashes match 
it will then use the cache instead of trying to install everything again from scratch
** **IMPORTANT:** It is for these types of use cases that comitting the `-lock` file can 
be extremely helpful.
* We install all the dependencies with `pnpm` and with the `--frozen-lockfile` flag so 
that it doesn't try to overwrite something on the lockfile.
** _Note:_ `pnpm i --frozen-lockfile` is recommended to be used on CI/CD pipelines, to ensure 
consistency. The command install dependencies without modifying the `pnpm-lock.yaml` 
file.
* We then run **ESLint** by leveraging the scripts that are also leveraged by xref:husky.adoc[Husky].
* We then run a **build** to check that all the code is okay and nothing broke.
* We then leverage a script called `debug-check.js` that's under a `scripts` folder at 
the web app folder. This is used by both _Husky_ and now the CI/CD pipeline in order to 
check for `console.log` or `debugger` statements in the code and failing if it does find 
them.
* We then run a **check for outdated packages**, but we fail silently, still the table of 
all dependencies that are outdated will show in the pipeline summary.
* We will then run a script that **checks for package versions that have vulnerabilities** 
in them that were found.

=== Deploy

Steps:

* This will depend on the previous <<ci-2, CI>> step, if it doesn't fail then deploy will 
run normally
* Run checkout recipe
* Run pnpm recipe
* Run node repice, **with a specific node version**, we are aiming at staying up to 
date and with the most modern yet in _Active LTS_ or _Maintenance LTS_ from Node, 
and so we setup node with **Version 22**. Same setup to take from cache for modules 
if we have them there.
* Install dependencies with frozen-lockfile
* Build the project, this isn't for a simple validation check, but so that we get a 
`dist` folder that can then be published to the web.
