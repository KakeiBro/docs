[source, bash]
----
#!/bin/bash

# Variables
SERVICE_NAME=$SERVICE_NAME <1>
REGION=$REGION
PROJECT_ID=$PROJECT_ID
REPOSITORY_NAME=$REPOSITORY_NAME
IMAGE_NAME=$IMAGE_NAME

# Authenticate to Google Cloud (already done in the workflow, but included for completeness)
echo "Authenticating to Google Cloud..."
gcloud auth activate-service-account --key-file=$GOOGLE_APPLICATION_CREDENTIALS <2>

# Get the list of tags for the specified package, sorted by creation time in descending order
echo "Fetching tags for package..."
IMAGES=$(gcloud artifacts docker images list $REPOSITORY_NAME \ <4>
  --project=$PROJECT_ID \
  --location=$REGION \ <3>
  --format="value(DIGEST)" \ <5>
  --sort-by="~CREATE_TIME") <6>

# Convert the list of digests into an array
IMAGE_ARRAY=($IMAGES)

# Check if there are more than 2 images
if [ ${#IMAGE_ARRAY[@]} -gt 2 ]; then
  echo "Found ${#IMAGE_ARRAY[@]} images. Keeping the 2 most recent ones and deleting the rest..."

  # Loop through the tags, skipping the first 2 (most recent)
  for ((i=2; i<${#IMAGE_ARRAY[@]}; i++)); do
    DIGEST=${IMAGE_ARRAY[$i]}
    echo "Deleting image: $DIGEST"
    gcloud artifacts docker images delete "$REPOSITORY_NAME@$DIGEST" \ <7>
      --project=$PROJECT_ID \
      --location=$REGION \
      --quiet \
      --force-delete-tags
  done
else
  echo "Only ${#IMAGE_ARRAY[@]} tags found. No cleanup needed."
fi

echo "Cleanup complete!"
----
<1> Just so that the script gets access to the upstream env variables we need to make 
this assignment (for all variables).
<2> Luckily, we get access to the env variable with our google credentials, this 
should be populated already due to the previous steps that ran before this script. 
(In case this needs to be leveraged elsewhere, be sure to be logged into GCloud before 
trying to run this script)
<3> Unlike the `$ARTIFACT_REGISTRY` value that references both the region plus the 
actual domain, this is simply the region section of that.
<4> The same repository name that we have under _secrets_, this is the root that 
we created at the _Artifact Registry_ level.
<5> Part of glcloud's https://cloud.google.com/sdk/gcloud/reference/artifacts/docker/images/list[docs] 
references the idea that we can both just get specific fields of the whole payload, in this case 
we just want the digest to point specifically to it.
<6> Part of conventions by gcloud allows for us to feed a sorting field reference and 
with the tilde `~` we can invert the default _ascending_ so that we start from the 2nd newest onwards. 
<7> And so we will get the digest's for each docker image that's hosted, and we will only 
keep the last 2 images (the last one that was pushed and that should be on our Cloud Run 
instance and a previous version as a backup). Any other image will be deleted, and no trace 
should be left behind.
+
_Note:_ One small thing to take into account is that layers might be present at the 
registy level, and other things specific to the inner-workings, so you might see 
listed way more artifacts than the titular 2 we decided to keep. But **don't try to 
delete them manually**, there are alerts for some of them also in place, but if we delete 
things without really knowing what they are, we might break other services. However, 
it's worth noting that xref:ci-cd-pipelines.adoc#build-push-action-provenance[provenance] 
is part of what generates untagged extra artifacts.